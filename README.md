# By-CRR Solu√ß√µes em Tecnologia AI - Assistente de IA com Ollama

Um assistente de intelig√™ncia artificial aut√¥nomo que utiliza Ollama para processamento local de linguagem natural. Por padr√£o, o aplicativo utiliza o modelo `phi4` (quando dispon√≠vel no Ollama). Caso n√£o esteja dispon√≠vel, faz fallback para `phi3`. Sistema foi experimentado usando dados publicos de empresa de saneamento e do SUS para explorar capacidade de respostas e possivel aplica√ß√£o em empresas do ramo de saneamento e saude p√∫blica

## üöÄ Instala√ß√£o

### M√©todo 1: Instalador Gr√°fico (Recomendado)
1. Execute `install.bat`
2. O instalador com interface gr√°fica ser√° aberto
3. Clique em "INICIAR INSTALA√á√ÉO" e aguarde o progresso
4. O atalho ser√° criado automaticamente na √°rea de trabalho com √≠cone (hex√°gono azul)

### M√©todo 2: Instala√ß√£o Manual
1. Certifique-se de ter Python 3.8+ instalado
2. Instale as depend√™ncias:
   ```bash
   pip install requests
   ```
3. Execute o sistema:
   ```bash
   python warpclone_gui.py
   ```

### M√©todo 3: Execut√°vel
1. Execute `build_executable.py`
2. O execut√°vel `ByCRR_AI.exe` ser√° criado na pasta `dist/`
3. Crie um atalho manualmente para o execut√°vel

## üìñ Como Usar

### Modo Interativo
```bash
python warpclone.py
```

Depois digite suas tarefas:
- "Liste todos os arquivos python nesta pasta"
- "Crie um script que imprime 'Hello World'"
- "Analise o conte√∫do do arquivo config.json"
- "Execute o comando 'ls -la' e mostre os resultados"

### Modo Comando Direto
```bash
python warpclone.py "crie um arquivo teste.txt com o conte√∫do 'Ol√° Mundo'"
```

## üéØ Capacidades

O By-CRR Solu√ß√µes em Tecnologia AI pode:

### Executar Comandos do Sistema
- Qualquer comando bash/shell
- Com timeout de seguran√ßa (30s)

### Manipular Arquivos
- Ler arquivos existentes
- Criar novos arquivos
- Modificar arquivos existentes

### Navegar Diret√≥rios
- Listar conte√∫do de pastas
- Obter informa√ß√µes sobre arquivos

### Pesquisas Avan√ßadas
- **Pesquisa no sistema de arquivos**: Busca por padr√µes (ex: `*.py`, `*.txt`)
- **Pesquisa de conte√∫do**: Busca texto dentro de arquivos
- **Pesquisa web**: Busca informa√ß√µes na internet (simulado)
- **An√°lise de sistema**: Analisa o ambiente e recursos

### Sistema de Aprendizado
- **Mem√≥ria persistente**: Armazena tarefas e resultados
- **Padr√µes de aprendizado**: Identifica padr√µes de sucesso
- **Hist√≥rico de comandos**: Registra todas as a√ß√µes
- **Melhoria cont√≠nua**: Aprende com experi√™ncias passadas

### Racioc√≠nio Aut√¥nomo
- Decide quais a√ß√µes tomar
- Executa m√∫ltiplas etapas
- Aprende com resultados anteriores
- Melhora o desempenho com o tempo

## üîß Exemplos de Tarefas

### Criar e executar um script
```bash
python warpclone.py "crie um script Python que calcula fatorial e execute para n=5"
```

### An√°lise de arquivos
```bash
python warpclone.py "leia todos os arquivos .py nesta pasta e me d√™ um resumo"
```

### Automatiza√ß√£o
```bash
python warpclone.py "organize os arquivos desta pasta por extens√£o"
```

### Busca de informa√ß√µes
```bash
python warpclone.py "encontre todos os arquivos modificados hoje"
```

### Pesquisas avan√ßadas
```bash
# Pesquisar arquivos por padr√£o
python warpclone.py "pesquise todos os arquivos Python nesta pasta"

# Pesquisar conte√∫do dentro de arquivos
python warpclone.py "pesquise por 'def main' nos arquivos Python"

# An√°lise de sistema
python warpclone.py "analise o sistema e me diga o que encontrou"

# Busca na web (simulada)
python warpclone.py "pesquise na web sobre Python decorators"
```

## ‚öôÔ∏è Configura√ß√£o Avan√ßada

### Alterar o modelo LLM (phi4 por padr√£o)
- Edite `warpclone_config.json` e ajuste:
  - `llm_model`: nome do modelo Ollama (ex.: `phi4`, `phi3`, `llama3`)
  - `ollama_url`: normalmente `http://localhost:11434/api/chat`
  
Exemplo:
```json
{
  "llm_model": "phi4",
  "ollama_url": "http://localhost:11434/api/chat"
}
```

### Modo Offline e Autostart do Ollama
- Para operar sem LLM e evitar avisos, ative o modo offline:
```json
{
  "offline_mode": true,
  "ollama_autostart": false,
  "ollama_check_interval_sec": 30
}
```
- `offline_mode`: desativa tentativas de conex√£o ao Ollama e usa heur√≠sticas locais.
- `ollama_autostart`: se `true`, tenta iniciar `ollama serve` automaticamente quando estiver dispon√≠vel.
- `ollama_check_interval_sec`: cache da verifica√ß√£o de disponibilidade para evitar checagens repetidas.

### Ajustar timeout de comandos
No m√©todo execute_command, modifique:
```python
timeout=30  # segundos
```

### Aumentar hist√≥rico de contexto
No m√©todo call_ollama, modifique:
```python
for msg in self.conversation_history[-5:]:  # √öltimas 5 mensagens
```

## üõ°Ô∏è Seguran√ßa

‚ö†Ô∏è **IMPORTANTE**: Este sistema executa comandos com suas permiss√µes. Use com cuidado!

Recomenda√ß√µes:
- Rode em ambiente isolado/sandbox
- Revise comandos antes de executar em produ√ß√£o
- Use usu√°rio com permiss√µes limitadas
- Monitore as a√ß√µes executadas

## üêõ Troubleshooting

### "Erro ao comunicar com Ollama"
- Verifique se o Ollama est√° rodando: `ollama serve`
- Confirme a porta: deve ser 11434

### "Modelo n√£o encontrado"
```bash
ollama pull phi4
# se falhar, tente
ollama pull phi3
```

### Respostas lentas
- Phi-3 √© um modelo pequeno e r√°pido
- Se ainda estiver lento, considere usar GPU
- Verifique recursos do sistema

### O agente n√£o executa a√ß√µes
- O modelo pode n√£o estar seguindo o formato JSON
- Tente reformular a tarefa de forma mais espec√≠fica
- Considere usar um modelo maior (llama2, mistral)

## üîÑ Melhorias Futuras
- [x] Interface web
- [x] Hist√≥rico persistente
- [x] Integra√ß√£o com APIs externas
- [x] Sistema de plugins
- [ ] M√∫ltiplos agentes cooperativos
- [x] Mem√≥ria de longo prazo
- [ ] Integra√ß√£o com banco de dados
- [ ] Interface gr√°fica
- [ ] Suporte para m√∫ltiplos modelos de IA
- [ ] Sistema de notifica√ß√µes

## üìù Estrutura do Projeto
```
BY-CRR AI/
‚îú‚îÄ‚îÄ warpclone.py              # Script principal
‚îú‚îÄ‚îÄ requirements.txt          # Depend√™ncias
‚îú‚îÄ‚îÄ README.md                 # Este arquivo
‚îú‚îÄ‚îÄ warpclone_memory/         # Diret√≥rio de mem√≥ria
‚îÇ   ‚îú‚îÄ‚îÄ memory.json          # Mem√≥ria persistente
‚îÇ   ‚îî‚îÄ‚îÄ learning_patterns.json # Padr√µes de aprendizado
‚îî‚îÄ‚îÄ warpclone_logs/          # Diret√≥rio de logs
    ‚îî‚îÄ‚îÄ command_history.json # Hist√≥rico de comandos
```

## üí° Dicas de Uso
- **Seja espec√≠fico**: "Crie um servidor HTTP na porta 8000" √© melhor que "crie um servidor"
- **Tarefas complexas**: O agente pode executar at√© 10 itera√ß√µes. Divida tarefas muito complexas.
- **Contexto**: O agente lembra das √∫ltimas 5 intera√ß√µes para manter contexto.
- **Erros**: Se algo falhar, o agente tentar√° corrigir automaticamente.
- **Pesquisas**: Use termos claros para pesquisas de arquivos e conte√∫do
- **Aprendizado**: O sistema melhora com o uso - quanto mais voc√™ usar, mais inteligente ele fica
- **Mem√≥ria**: As mem√≥rias s√£o salvas automaticamente e persistem entre sess√µes

## üóëÔ∏è Desinstala√ß√£o

### M√©todo 1: Desinstalador Gr√°fico (Recomendado)
1. Execute `uninstall.bat`
2. O desinstalador com interface gr√°fica ser√° aberto
3. Escolha se deseja remover tamb√©m seus dados pessoais
4. Clique em "DESINSTALAR"

### M√©todo 2: Desinstala√ß√£o Manual
1. Remova o atalho da √°rea de trabalho: `By-CRR Solu√ß√µes em Tecnologia AI.lnk`
2. Delete a pasta do projeto
3. Remova as pastas `warpclone_memory` e `warpclone_logs` (opcional)
4. Delete o arquivo `warpclone_config.json` (opcional)

## üìÑ Licen√ßa
Livre para uso pessoal e educacional.
